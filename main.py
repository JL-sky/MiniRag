from chunk_file import ReadFiles
from vector import VectorStore
from embedding import SentenceTransformerEmbedding
from chat_model  import ChatModel
from IPython.display import display, Code, Markdown
from bs4 import BeautifulSoup

def run_mini_rag(question: str, knowledge_base_path: str, k: int = 1) -> str:
  """
  è¿è¡Œä¸€ä¸ªç®€åŒ–ç‰ˆçš„RAGé¡¹ç›®ã€‚
  
  :param question: ç”¨æˆ·æå‡ºçš„é—®é¢˜
  :param knowledge_base_path: çŸ¥è¯†åº“çš„è·¯å¾„ï¼ŒåŒ…å«æ–‡æ¡£çš„æ–‡ä»¶å¤¹è·¯å¾„
  :param api_key: OpenAI APIå¯†é’¥ï¼Œç”¨äºè°ƒç”¨GPT-4oæ¨¡å‹
  :param k: è¿”å›ä¸é—®é¢˜æœ€ç›¸å…³çš„kä¸ªæ–‡æ¡£ç‰‡æ®µï¼Œé»˜è®¤ä¸º1
  :return: è¿”å›GPT-4oæ¨¡å‹ç”Ÿæˆçš„å›ç­”
  """
  # åŠ è½½å¹¶åˆ‡åˆ†æ–‡æ¡£
  docs = ReadFiles(knowledge_base_path).get_content(max_token_len=600, cover_content=150)
  vector = VectorStore(docs)
  # ä½¿ç”¨ SentenceTransformer Embedding æ¨¡å‹å¯¹æ–‡æ¡£è¿›è¡Œå‘é‡åŒ–
  embedding=SentenceTransformerEmbedding(is_api=False)
  # è·å–æ–‡æ¡£å‘é‡å¹¶å­˜å‚¨
  vector.get_vector(embedding)
  # æ‰“å°æ–‡æ¡£å‘é‡
  print(vector.vectors)
  # æŒä¹…åŒ–å­˜å‚¨åˆ°æœ¬åœ°
  vector.persist('storage')
  # åœ¨æ•°æ®åº“ä¸­æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ
  content = vector.query(question, EmbeddingModel=embedding, k=1)[0]
  # ä½¿ç”¨ Qwen3 æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
  chat = ChatModel() 
  answer=chat.chat(question, [], content)
  soup =BeautifulSoup(answer, 'html.parser')
  # print("æ¨¡å‹å›ç­”:", answer)
  think_content = soup.find('think').text.strip() if soup.find('think') else None
  print ("ğŸ¤– \næ€è€ƒç»“æœ:\n", think_content)
  print("ğŸ¤– \næ­£åœ¨ç”Ÿæˆå›ç­”...\n")
  non_think_content = []
  for element in soup.children:
      if element.name != 'think' and str(element).strip():
          non_think_content.append(str(element).strip())
  print('\n'.join(non_think_content))
  return non_think_content

if __name__ == '__main__':
  # ç”¨æˆ·æå‡ºé—®é¢˜
  question = 'OpenAIçš„åˆ†è¯æŠ€æœ¯ç»å†äº†å“ªäº›è¿­ä»£'
  knowledge_base_path = './data'
  run_mini_rag(question, knowledge_base_path)